---
title: "Modelling US Healthcare Utilisation and Expenditure using 2012 Medical Expenditure Panel Survey Data"
subtitle: "SMM634 Individual Assignment 2025-26"
author:
  - "Benjamin Evans"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    number_sections: true
    toc: true
    latex_engine: xelatex
    citation_package: natbib
    keep_tex: true
  # runtime: shiny
  bookdown::html_document2:
    self_contained: true
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: show
    theme: 
      bootswatch: lumen
      base_font:
        google: "Source Sans Pro"
    highlight: tango
    df_print: paged
# fontsize: 10pt
bibliography: references.bib
---

```{=latex}
\newpage
```

```{=html}
<h1>Information</h1>

This R Markdown document was created as part of an individual assignment for SMM634 at Bayes Business School, City St George's, University of London in Term 1 2025-26.

```

```{r asthetic-header, include=FALSE, echo=FALSE}
# ==============================================================================
# SMM047 Probability and Mathematical Statistics (Subject CS1)
# Individual Coursework
# Author: Benjamin Evans
# Professor:    Professor Rosalba Radice
# Institution:  Bayes Business School - City St George's, University of London
# Date:         05/Dec/2025
# Description:  Term 1 individual project for Analytics Methods for Business. 
# Dependencies:
#   - TBC
# ==============================================================================
# 
# References
# file:///Users/eddy/Downloads/Copula%20Additive%20Distributional%20Regression%20Using%20R_25_11_25_12_44_26.pdf
# 
# 
# 
```

```{r setup-knitr, include=FALSE, echo=FALSE}
#----------------------- Initial setup (knitr settings) -----------------------#
dir.create("fig", showWarnings = FALSE)

# Defaults common to all outputs
knitr::opts_chunk$set(
  echo     = TRUE,
  message  = FALSE,
  warning  = FALSE,
  fig.align = "center",
  out.width = "100%",
  fig.path  = "fig/",
  dpi       = 300
)

# Output-specific settings
if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(
    fig.width = 6,
    fig.height = 4,
    dev = "pdf",
    fig.pos = "ht",
    out.extra = ""
  )
} else {
  knitr::opts_chunk$set(
    fig.width = 6,
    fig.height = 4,
    dev = "svglite"  # or "png"
  )
}

if (knitr::is_latex_output()) {
  cat("\\renewcommand{\\arraystretch}{1.1}\n")
}
```

```{r setup-qol, include=FALSE, echo=FALSE}
#----------------------------- Clean environment ------------------------------#
rm(list = ls()) # Remove all objects
graphics.off() # Close all graphical devices
cat("\014") # Clean console
```

```{r load-dependencies, include=FALSE, echo=FALSE}
#------------------- Load dependencies / external libraries -------------------#
library(tidyverse)
library(glmnet)  
library(MASS)    
library(ROCR)    
library(GJRM)    # Copula

library(broom)

library(ggplot2)
library(dplyr)
library(patchwork)
library(kableExtra)
library(stringr)

# for custom functions
library(clipr) # for banner_comment function qol to annotate code
```

```{r setup-html-app, echo=FALSE, results='asis', eval=knitr::is_html_output(), purl=FALSE}
#------------------------------ HTML link to app ------------------------------#
library(htmltools)

div(style = "background-color: #f8f9fa; padding: 20px; border: 1px solid #e9ecef; border-radius: 5px; text-align: center; margin-bottom: 30px;",
  h3("Bonus Interactive Dashboard Available"),
  p("This static report is accompanied by a live R Shiny dashboard allowing one to test different tickers, exclude specific outlier periods, and adjust bootstrap simulation parameters."),
  a(href = "https://3enji.shinyapps.io/SMM047-202526-Group07-Dashboard/", 
    target = "_blank",
    class = "btn btn-primary", 
    style = "background-color: #007bff; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold;",
    "Update this link!!!! Launch Interactive Dashboard")
)
```

## Custom functions (to hide in final version)

```{r custom-functions, include=FALSE, echo=FALSE}
#---------------------------- Custom QOL functions ----------------------------#
#####################################
# function: banner comments (used to to section up code)
# Usage: banner_comment("Element 1: data cleaning") -> then ctrl + v (or cmd+v)
#####################################
banner_comment <- function(text, width = 80, border = "#", fill = "-") {
  txt <- paste0(" ", text, " ")
  inner_width <- width - 2 * nchar(border)
  banner_string <- ""
  
  if (inner_width <= nchar(txt)) {
    banner_string <- paste0(border, txt, border)
  } else {
    pad_total <- inner_width - nchar(txt)
    pad_left <- pad_total %/% 2
    pad_right <- pad_total - pad_left
    
    banner_string <- paste0(
      border,
      strrep(fill, pad_left),
      txt,
      strrep(fill, pad_right),
      border
    )
  }
  
  cat(banner_string, "\n")
  # copy banner to allow direct pasting (requires clipr)
  clipr::write_clip(banner_string)
  # avoid [1] when printing if want to manually copy
  invisible(banner_string)
}
#####################################
# function: format p-values for text 
# Usage (in-line): `r format_p_vals(ad_test_result$p.value)`
# Usage (console): format_p_vals(ad_test_result$p.value)
#####################################
format_p_vals <- function(p) {
  if (length(p) != 1L || is.na(p)) {
    stop("Error! p must be a single non-missing value")
  }
  if (p > 1) {
    stop("Error! Value greater than 1")
  }
  if (p < 0) {
    stop("Error! Value less than 0")
  }

  if (p >= 0.01) {
    paste0("= ", formatC(p, format = "f", digits = 2))
  } else if (p >= 0.001) {
    paste0("= ", formatC(p, format = "f", digits = 3))
  } else {
    "< 0.001"
  }
}
#####################################
# function: format confidence intervals for tables & text 
# Usage (in-line): `r format_interval(el2_ci_normal_95[1], el2_ci_normal_95[2])`
# Usage (console): format_interval(el2_ci_normal_95[1], el2_ci_normal_95[2])
#####################################
format_interval <- function(lower, upper, digits=3) {
  paste0("[", 
       formatC(lower, format = "f", digits = digits), ", ",
       formatC(upper, format = "f", digits = digits), 
       "]")
}
#####################################
# function: format variable names in green in latex (& normal code in html)
# Usage (in-line): TBI `r format_var_name("dvisit")`
#####################################
format_var_name <- function(x) {
  if (knitr::is_latex_output()) {
    # replace all "_" with "\_" in va name
    paste0("\\greentt{", gsub("_", "\\\\_", x), "}")
  } else {
    # change to paste0("<code style='color:green'>", x, "</code>") for green
    # paste0("`", x, "`")
    paste0("<code style='color:green'>", x, "</code>")
  }
}
#####################################
# function: quick format (~legacy qol function)
#####################################
fmt <- function(n, digits=0, big_mark = ",") {
  formatC(n, format = "f", digits = digits, big.mark = big_mark)
}
```

```{r Load-data, include=FALSE, echo=FALSE}
#---------------------------- Download / Load data ----------------------------#
meps <- meps_raw <- readr::read_delim("meps.txt", delim = "\t")

meps %>%
  summarise(across(where(is.numeric),
                   ~ paste0(min(.x, na.rm = TRUE), " – ", max(.x, na.rm = TRUE))))

edu_levels <- c(
  "Less than High School", 
  "High School Graduate", 
  "College+"
)

edu_levels_detailed <- c(
  "Less than High School", 
  "High School Graduate", 
  "Some College", 
  "College Graduate", 
  "Post-Graduate"
)

meps <- meps_raw %>%
  mutate(
    # --- 1. standard ordinal / nominal factors ---
    general = factor(
      general,
      levels = 1:5,
      labels = c("Excellent", "VGood", "Good", "Fair", "Poor")    
    ),
    mental = factor(
      mental,
      levels = 1:5,
      labels = c("Excellent", "VGood", "Good", "Fair", "Poor")
    ),
    region = factor(
      region,
      levels = 1:4,
      labels = c("Northeast", "Midwest", "South", "West")
    ),
    ethnicity = factor(
      ethnicity,
      levels = 1:4,
      labels = c("White", "Black", "Native American", "Others")
    ),
    # --- 2. binary factors (0/1 mappings) ---
    gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male")),
    hypertension = factor(
      hypertension,
      levels = c(0, 1),
      labels = c("No", "Yes")
    ),
    hyperlipidemia = factor(
      hyperlipidemia,
      levels = c(0, 1),
      labels = c("No", "Yes")
    ),
    # --- education categories 
    # add 'education_cat' for categorical analysis.
    education_cat = case_when(
      education < 12 ~ "Less than High School",
      education >= 12 & education < 13 ~ "High School Graduate",
      education >= 13 ~ "College +",
      TRUE ~ NA_character_
    ),
    education_cat = factor(
      education_cat,
      levels = c("Less than High School", "High School Graduate", "College +")
    ),
    education_cat_detailed = case_when(
      education < 12 ~ "Less than High School",
      education >= 12 & education < 13 ~ "High School Graduate", # Catches 12 and 12.5
      education >= 13 & education < 16 ~ "Some College",       # Catches 13 to 15.9
      education >= 16 & education < 17 ~ "College Graduate",    # Catches 16 and 16.5
      education >= 17 ~ "Post-Graduate",
      TRUE ~ NA_character_
    ),
    # Ensure levels are ordered by SES for proper reference group in regression
    education_cat_detailed = factor(
      education_cat_detailed,
      levels = c(
        "Less than High School", 
        "High School Graduate", 
        "Some College", 
        "College Graduate", 
        "Post-Graduate"
      )
    ),
    # adds has doctors expense check 
    has_expense = ifelse(dvexpend > 0, 1, 0)
  )

check_levels <- list(
  Gen = levels(meps$general),
  Men = levels(meps$mental),
  Eth = levels(meps$ethnicity),
  Reg = levels(meps$region),
  Edu = levels(meps$education_cat_detailed)
)

print(check_levels)

meps_all <- meps
# exclude individuals with missing information
meps_clean <- na.omit(meps_all)

# filter out individuals with zero consultations and zero associated expenditures
# Note: Filtering dvexpend > 0 handles this and prepares for Gamma GLM
meps_pos <- meps_clean %>% filter(dvexpend > 0)

zero_visit_zero_cost <- sum(meps_clean$dvisit == 0 & meps_clean$dvexpend == 0)
pos_visit_zero_cost  <- sum(meps_clean$dvisit > 0 & meps_clean$dvexpend == 0) 
zero_visit_pos_cost  <- sum(meps_clean$dvisit == 0 & meps_clean$dvexpend > 0) 
# print(paste("Patients with 0 visits AND 0 cost:", zero_visit_zero_cost))
# print(paste("Patients with >0 visits but 0 cost:", pos_visit_zero_cost))
# print(paste("Patients with 0 visits but >0 cost:", zero_visit_pos_cost))


mean_visit <- mean(meps_clean$dvisit)
var_visit  <- var(meps_clean$dvisit)

n_total_raw <- nrow(meps_raw)
n_full      <- nrow(meps_clean)
n_pos       <- nrow(meps_pos) 
n_dropped   <- n_total_raw - n_full
n_zeros     <- n_full - n_pos

# calculate Zero-Cost stats based on the clean dataset
n_zero_cost <- sum(meps_clean$dvexpend == 0)
pct_zero    <- (n_zero_cost / n_full) * 100

# stats for visits specifically
n_zero_visit <- sum(meps_clean$dvisit == 0)

# verify clean
glimpse(meps)
summary(meps)
glimpse(meps_clean)
summary(meps_clean)
```


```{r check-na, echo=FALSE, purl=FALSE, include=FALSE}
#---------------------------- Check for NaN values ----------------------------#
# quick check for NA
if (TRUE == TRUE) {
  # get counts and percentages
  check <- meps
  na_summary <- check %>%
    summarise(across(everything(), ~ sum(is.na(.)))) %>%
    pivot_longer(
      cols = everything(),
      names_to = "Variable",
      values_to = "Missing Count"
    ) %>%
    mutate(
      `Missing %` = (`Missing Count` / nrow(check)) * 100,
      `Missing %` = paste0(formatC(`Missing %`, format = "f", digits = 2), "%")
    ) %>%
    # filter to show only variables with missing data
    # filter(`Missing Count` > 0) %>%
    arrange(desc(`Missing Count`)) 

  # table
  na_summary %>%
    kbl(
      caption = "Summary of missing values (NaN) by variable in the raw dataset.",
      booktabs = TRUE,
      align = c("l", "r", "r")
    ) %>%
    kable_styling(latex_options = "hold_position") %>%
    row_spec(0, bold = TRUE)
}
```

```{r intro-table, echo = FALSE}
get_cat_compare <- function(df_full, df_pos, var_name, display_name) {
  # Calculate proportions
  get_props <- function(d, v) {
    t <- table(d[[v]])
    formatC(as.numeric(prop.table(t)), format = "f", digits = 2)
  }
  # get labels and values
  clean_labels <- gsub("^[0-9]:\\s*", "", names(table(df_full[[var_name]])))
  # Tibble with one row per category level
  tibble(
    Variable = format_var_name(display_name),
    Description = clean_labels,
    `Full Sample` = get_props(df_full, var_name),
    `Positive Only` = get_props(df_pos, var_name)
  )
}
# var_summary <- tibble(
#   Variable = names(meps_raw),
#   Type = c("Ordinal Factor", "Ordinal Factor", "Continuous", "Continuous", "Continuous", 
#            "Binary Factor", "Nominal Factor", "Categorical", "Nominal Factor", 
#            "Binary Factor", "Binary Factor", "Count", "Count", "Continuous", "Continuous"),
#   `Raw Range/Levels` = c("1-5", "1-5", "9.4-68.2", "0-Max", "18-65", "0/1", "1-4", "0-17", "1-4", "0/1", "0/1", "0-29", "0-29", "0-Max", "0-Max"),
#   `Analysis Treatment` = c("Factor (Ref: Exc)", "Factor (Ref: Exc)", "Linear/Spline", "Linear (Log?)", "Linear", 
#                            "Factor (Ref: Female)", "Factor (Ref: White)", "Factor (17 levels)", "Factor (Ref: NE)", 
#                            "Factor (Ref: No)", "Factor (Ref: No)", "Response (Count)", "Response (Count)", "Response (Gamma)", "Response")
# )
# 
# # Display Table
# kbl(
#   var_summary, 
#   caption = "MEPS Variable Definitions and Analysis Treatment",
#   booktabs = TRUE
#   ) %>%
#   kable_styling(
#     bootstrap_options = c("striped", "hover")
# )
```

# Introduction

Healthcare expenditure represents a substantial share of economic activity in
many high-income countries. In the United States, national health spending is
projected to reach 20.3% of Gross Domestic Product (GDP) by 2033, up from 17.6%
in 2023 [@Keehan2025]. Accurately modelling healthcare utilisation and
expenditure is essential for forecasting budgetary pressures, planning service
capacity, and designing policies that can meet the needs of an ageing population
in an efficient and sustainable way.

The Agency for Healthcare Research and Quality (AHRQ) publishes the Medical
Expenditure Panel Survey (MEPS), which provides detailed information on
healthcare utilisation, associated expenditures, insurance coverage, and
socio-demographic characteristics, all standardised to represent a full calendar
year for each respondent [@ahrq_meps_download_2025].  These data enable one to
quantify how healthcare use and costs vary across population groups and to
identify factors associated with higher or lower spending, which is valuable for
targeting interventions and evaluating potential policy reforms.

From a modelling perspective, healthcare expenditure data pose several
distributional challenges: they are non-negative, highly right-skewed (a few
patients incur massive costs), and contain a large proportion of individuals
with no recorded healthcare useage. Understanding both whether individuals access
healthcare and, conditional on doing so, how much is spent is crucial for
characterising demand and the resulting financial burden.  This report uses 2012
MEPS data to investigate two primary outcomes related to physician services: the
number of doctor consultations (`r format_var_name("dvisit")`), capturing
healthcare utilisation, and the annual expenditures on doctor visits 
(`r format_var_name("dvexpend")`), capturing the associated financial cost.

# Methods 

The 2012 MEPS dataset contains `r format(n_total_raw, big.mark=",")`
observations on US adults aged 18–65. Tables \@ref(tab:initialOverviewCat) &
\@ref(tab:initialOverviewNum) show the categorical and quantitative variables
for both the full sample and a subsample of individuals with
positive expenditure.

The covariates include demographic characteristics 
(`r format_var_name("age")`, `r format_var_name("gender")`,
`r format_var_name("ethnicity")`, `r format_var_name("region")`, 
and `r format_var_name("education")`), socioeconomic status 
(`r format_var_name("income")`),
health indicators (`r format_var_name("BMI")`, self-reported 
health `r format_var_name("general")` and `r format_var_name("mental")` health, 
`r format_var_name("hypertension")`, and `r format_var_name("hyperlipidemia")`) 
and healthcare utilisation beyond physician visits (number
of non-physician visits, `r format_var_name("ndvisit")`). 
`r format_var_name("ndvisit")` was retained because it
captures an underlying propensity to use healthcare services, whereas
non-physician expenditure (`r format_var_name("ndvexpend")`) was excluded to 
avoid 'leakage' from using one spending outcome to predict another.

```{r initialOverviewCat, echo=FALSE}
tab_cat_comp <- bind_rows(
  get_cat_compare(meps_clean, meps_pos, "gender", "gender"),
  get_cat_compare(meps_clean, meps_pos, "ethnicity", "ethnicity"),
  get_cat_compare(meps_clean, meps_pos, "education_cat_detailed", "education_cat_detailed"),
  get_cat_compare(meps_clean, meps_pos, "region", "region"),
  get_cat_compare(meps_clean, meps_pos, "hypertension", "hypertension"),
  get_cat_compare(meps_clean, meps_pos, "hyperlipidemia", "hyperlipidemia")
)

header_full <- linebreak(
  paste0("Full Sample\n(N = ", format(n_full, big.mark = ","), ")"),
  align = "c"
)

header_pos <- linebreak(
  paste0("Positive expenditure\n(N = ", format(n_pos, big.mark = ","), ")"),
  align = "c" 
)

# Render Categorical Table
# Note: We need to calculate linesep again because we have groups
cat_linesep <- rep("", nrow(tab_cat_comp))
group_ends <- cumsum(rle(as.character(tab_cat_comp$Variable))$lengths)
cat_linesep[group_ends] <- "\\addlinespace"
cat_linesep[nrow(tab_cat_comp)] <- ""

tab_cat_comp %>%
  kbl(
    caption = "Frequency of categorical variables in the full sample of patients and the subset of patients with positive healthcare expenditure.",
    col.names = c("Variable", "Category", header_full, header_pos),
    booktabs = TRUE,
    escape = FALSE,
    align = c("l", "l", "c", "c"),
    linesep = cat_linesep   # use the vector you calculated
  ) %>%
  kable_styling(latex_options = c("hold_position", "repeat_header")) %>%
  collapse_rows(columns = 1, valign = "middle", latex_hline = "major")

```

```{r initialOverviewNum, echo=FALSE}
get_num_compare <- function(
  df_full, 
  df_pos, 
  var_name, 
  display_name, 
  digits = 2, 
  big_mark = "",
  show_mean_se = FALSE
) {
  
  # calculate stat based on Toggle
  calc_stat <- function(d, v) {
    x <- d[[v]]
    
    if (show_mean_se) {
      # --- Calculate Mean (SE) ---
      m <- mean(x, na.rm = TRUE)
      s <- sd(x, na.rm = TRUE)
      n_obs <- sum(!is.na(x))
      se <- s / sqrt(n_obs)
      
      return(paste0(fmt(m, digits=digits), " (", fmt(s, digits=digits), ")"))
      
    } else {
      # calculate median & [IQR]
      med <- median(x, na.rm = TRUE)
      q1 <- quantile(x, 0.25, na.rm = TRUE)
      q3 <- quantile(x, 0.75, na.rm = TRUE)
      iqr <- 
        paste0(" [", fmt(q1, digits=digits), " – ", fmt(q3, digits=digits), "]")
      
      return(paste0(fmt(med, digits=digits), iqr))
    }
  }
  
  # for both groups
  val_full <- calc_stat(df_full, var_name)
  val_pos  <- calc_stat(df_pos, var_name)
  
  # build tibble w. generic column names
  tibble(
    Variable = format_var_name(var_name),
    Description = display_name,
    `Full Sample` = val_full,
    `Positive Only` = val_pos
  )
}

tab_num_comp <- bind_rows(
  get_num_compare(meps_clean, meps_pos, "bmi", "Body mass index", digits=2, show_mean_se = TRUE),
  get_num_compare(meps_clean, meps_pos, "age", "Age (years)", digits=2, show_mean_se = TRUE),
  get_num_compare(meps_clean, meps_pos, "education", "Education (no. of years)", digits=2, show_mean_se = TRUE),
  get_num_compare(meps_clean, meps_pos, "income", "Income (USD)", digits=0, big_mark=",", show_mean_se = TRUE),
  
  # utilisation
  get_num_compare(meps_clean, meps_pos, "dvisit", "Doctor visits", digits=2, show_mean_se = TRUE),
  get_num_compare(meps_clean, meps_pos, "ndvisit", "Non-doctor visits", digits=2, show_mean_se = TRUE),
  
  # expenditure
  get_num_compare(meps_clean, meps_pos, "dvexpend", "Doctor expenditure", digits=2, big_mark=",", show_mean_se = TRUE),
  get_num_compare(meps_clean, meps_pos, "ndvexpend", "Non-doctor expenditure", digits=2, big_mark=",", show_mean_se = TRUE)
)

tab_num_comp %>%
  kbl(
    caption = "Summary statistics, mean (standard deviation), for continuous variables in the full sample and among individuals with positive healthcare expenditure.",
    col.names = c("Variable", "Description", header_full, header_pos),
    booktabs = TRUE,
    escape = FALSE,
    linesep = c("","","", "\\addlinespace"),
    align = c("l", "l", "l", "l")
  ) %>%
  kable_styling(latex_options = "hold_position") %>%
  column_spec(1, width = "3cm") %>%
  row_spec(0, bold = TRUE)
```

```{r model-expenditure, echo=FALSE, include=FALSE}
#----------------------------- Expenditure model ------------------------------#
# Y as the Binary Outcome (0 or 1)
y_binary <- meps_clean$has_expense

# X - full dataset, not just positive
X_full <- model.matrix(
  has_expense ~ . -
    dvisit -
    dvexpend -
    # ndvisit -
    ndvexpend -
    education -
    education_cat,
  data = meps_clean
)[, -1]
# X_full <- model.matrix(
#   has_expense ~ age +
#     gender +
#     general +
#     mental +
#     ethnicity +
#     region +
#     hypertension +
#     bmi,
#   data = meps_clean
# )[, -1]

# Lasso with family = "binomial" (Logistic)
set.seed(123)
cv_lasso_binary <- cv.glmnet(X_full, y_binary, alpha = 1, family = "binomial")

# plot and check
# plot(cv_lasso_binary)
# print(coef(cv_lasso_binary, s = "lambda.1se"))

## now for second part
##
##
y_pos <- log(meps_pos$dvexpend)

X_pos <- model.matrix(
  dvexpend ~ . -
    dvisit -
    dvexpend -
    ndvisit - # playing with toggling this on and off - opted to include
    ndvexpend -
    has_expense -
    education -
    education_cat,
  data = meps_pos
)[, -1]

cv_lasso_pos <- cv.glmnet(X_pos, y_pos, alpha = 1)

# 5. Plot to see the error curve
# plot(cv_lasso_pos)

# 6. Check Active Predictors
# coef(cv_lasso_pos, s = "lambda.1se")

##############################
##############################
##############################
# Actual models
# Part 1: Probit for probability of any expense
# Using full dataset (meps_clean) to predict yes/no for everyone

m_part1 <- glm(
  has_expense ~ age +
    gender +
    bmi + # Selected by Lasso (unlike Part 2)
    general +
    mental + # Health Status
    region +
    ethnicity + # Demographics
    hypertension +
    hyperlipidemia + # Chronic Conditions
    income + # SES
    ndvisit + # Utilization Proxy
    education_cat_detailed, # Education
  family = binomial(link = "probit"), # Probit link
  data = meps_clean
)

summary(m_part1)

# Part 2: Gamma GLM for amount of expense if visiting
m_part2 <- glm(
  dvexpend ~ age +
    gender +
    general +
    mental + 
    region + 
    hypertension +
    hyperlipidemia + 
    income + 
    ndvisit + # utilisation proxy 
    education_cat_detailed, 
  family = Gamma(link = "log"),
  data = meps_pos
)

summary(m_part2)
```

```{=latex}
\newpage
```
## Health-care expenditure

Doctor-visit expenditure (`r format_var_name("dvexpend")`) is semi-continuous, 
with a mass of zero observations 
(`r format(n_zero_cost, big.mark=",")`, `r round(pct_zero,1)`%, incur zero 
healthcare costs) and a highly right-skewed distribution among positive values. 
To accommodate this structure, expenditure was modelled using a two-part 
framework.

A two-part model was selected (instead of a log-normal model) because it permits
the  decision to seek any care (extensive margin) and the intensity of
expenditure conditional on care (intensive margin) to depend on different
covariates and processes [@deb2018modeling]. The Gamma–log specification for the
second part is recommended for skewed medical cost data as it avoids the
retransformation bias associated with log-transformed models
[@blough1999modeling].

A Probit model is used to estimate the probability that an individual incurs any
doctor-visit expenditure and a Generalised Linear Model (GLM) with Gamma
distribution and log link models the amount spent, conditional on having
positive expenditure. The Gamma–log specification is recommended for medical
cost data because it captures heavy right skew without the retransformation bias
of log-normal models [@deb2018modeling]. For both parts, the choice of
covariates was informed by a preliminary Lasso regression using the full set of
candidate predictors, combined with prior domain knowledge. 
`r format_var_name("ndvisit")` was retained *a priori* in both parts as a proxy 
for latent propensity to use healthcare services. The final two-part model is
shown in Eq. \@ref(eq:expenProbit) & \@ref(eq:expenGamma).

\begin{align}
% Part 1: Probit 
\text{Part 1: }& \nonumber\\
\mathbb{P}(\text{dvexpend}_i > 0) &= \Phi(\eta_{1i}) \nonumber \\
\eta_{1i} &= \gamma_0 + \gamma_1 \text{Age}_i + \gamma_2 \text{Gender}_i + \gamma_3 \text{BMI}_i \nonumber \\
&\quad + \gamma_4 \text{Ethnicity}_i + \gamma_5 \text{Region}_i + \gamma_6 \text{Education}_i \nonumber \\
&\quad + \gamma_7 \text{General}_i + \gamma_8 \text{Mental}_i \nonumber \\
&\quad + \gamma_9 \text{Hypertension}_i + \gamma_{10} \text{Hyperlipidemia}_i \nonumber \\
&\quad + \gamma_{11} \text{Income}_i + \gamma_{12} \text{Ndvisit}_i (\#eq:expenProbit) \\[10pt]
% Part 2: Gamma
\text{Part 2: }& \nonumber\\
\mathbb{E}[\text{dvexpend}_i &\mid \text{dvexpend}_i > 0] = \mu_i \nonumber \\
\ln(\mu_i) &= \beta_0 + \beta_1 \text{Age}_i + \beta_2 \text{Gender}_i \nonumber \\
&\quad + \beta_3 \text{Region}_i + \beta_4 \text{Education}_i \nonumber \\
&\quad + \beta_5 \text{General}_i + \beta_6 \text{Mental}_i \nonumber \\
&\quad + \beta_7 \text{Hypertension}_i + \beta_8 \text{Hyperlipidemia}_i \nonumber \\
&\quad + \beta_9 \text{Income}_i + \beta_{10} \text{Ndvisit}_i (\#eq:expenGamma)
\end{align}
Where $\eta_{1i}$ is the linear predictor for the Probit model,
$\gamma_j$ are the coefficients for the Probit, and $\beta_j$ are the
coefficients for the Gamma GLM with a log link in the second part. 

```{r model-utilisation, echo = FALSE, include=FALSE}
#----------------------------- Utilisation model ------------------------------#
# Check for Overdispersion
mean_visits <- mean(meps_clean$dvisit)
var_visits <- var(meps_clean$dvisit)

# print(paste("Mean:", round(mean_visits, 2)))
# print(paste("Variance:", round(var_visits, 2)))
# if var >> mean -> Poisson invalid -> use Negative Binomial.

# using meps_clean to model 0s and positive counts.
X_count <- model.matrix(
  dvisit ~ . -
    dvisit -
    dvexpend -
    ndvexpend - 
    has_expense -
    education -
    education_cat, 
  data = meps_clean
)[, -1]

y_count <- meps_clean$dvisit

# lasso (poisson)
cv_lasso_count <- cv.glmnet(X_count, y_count, family = "poisson", alpha = 1)

# check selected variables
coef(cv_lasso_count, s = "lambda.1se")

# pois
m_pois <- glm(
  dvisit ~ age +
    gender +
    bmi +
    general +
    mental +
    ethnicity +
    region +
    hypertension +
    hyperlipidemia +
    income +
    ndvisit +
    education_cat_detailed,
  family = poisson(link = "log"),
  data = meps_clean
)

# Negative Binomial Model for Utilization
m_count <- glm.nb(
  dvisit ~ age +
    gender +
    bmi + 
    general +
    mental + 
    ethnicity +
    region + 
    hypertension +
    hyperlipidemia +
    income +
    ndvisit +
    education_cat_detailed, 
  data = meps_clean
)

summary(m_count)

# saving metrics to use in-line
# checking pois vs negative binomial through AIC
AIC(m_pois, m_count)
aic_pois   <- AIC(m_pois)
aic_negbin <- AIC(m_count)

dispersion_pois <- sum(residuals(m_pois, type = "pearson")^2) / m_pois$df.residual
# dispersion_pois

p_overdisp <- pchisq(m_pois$deviance, df = m_pois$df.residual, lower.tail = FALSE)
# p_overdisp
```

## Health-care utilisation

To analyse the frequency of physician encounters 
(`r format_var_name("dvisit")`), a Negative Binomial regression model was employed.
An initial inspection of the data revealed significant overdispersion, with the
variance of visits (`r round(var_visits, 1)`) far exceeding the mean 
(`r round(mean_visits, 1)`), violating the equidispersion assumption of the standard
Poisson model. 

As a formal benchmark, a Poisson regression model with the same covariates as
the final specification was fitted. A formal overdispersion test, comparing the
Poisson residual deviance to a $\chi^2$ distribution with the corresponding
residual degrees of freedom, yielded a Pearson dispersion of
`r round(dispersion_pois, 2)` and p `r format_p_vals(p_overdisp)`. The Poisson
model also had higher AIC than the Negative Binomial model
(AIC\(_\text{Poisson}\) = `r formatC(aic_pois, format = "f", digits = 0)` 
versus AIC\(_\text{NegBin}\) = `r formatC(aic_negbin, format = "f", digits = 0)`), further supporting 
the Negative Binomial specification.

Lasso screening and prior domain knowledge were used to inform covariate choice 
and the final specification for the healthcare utilisation model is given in Eq
\@ref(eq:utiliNegBin).

\begin{align}
% Count model: Negative Binomial
\text{Count model: } \text{dvisit}_i &\sim \text{NegBin}(\mu_i, \kappa) 
  \nonumber \\[4pt]
\log(\mu_i) &= \beta_0 
  + \beta_1 \text{Age}_i 
  + \beta_2 \text{Gender}_i 
  + \beta_3 \text{BMI}_i \nonumber \\
&\quad + \beta_4 \text{General}_i 
  + \beta_5 \text{Mental}_i \nonumber \\
&\quad + \beta_6 \text{Ethnicity}_i 
  + \beta_7 \text{Region}_i \nonumber \\
&\quad + \beta_8 \text{Hypertension}_i 
  + \beta_9 \text{Hyperlipidemia}_i \nonumber \\
&\quad + \beta_{10} \text{Income}_i 
  + \beta_{11} \text{Ndvisit}_i \nonumber \\
&\quad + \beta_{12} \text{Education}_i (\#eq:utiliNegBin)
\end{align}

In this report, the two-part expenditure model and the utilisation model for
doctor visits are estimated separately. Conditional on the observed covariates
(including `r format_var_name("ndvisit")`), no additional dependence between
visit counts and expenditure is modelled. This simplifying assumption allows
the frequency and severity components to be analysed individually, while still
providing a basis for future work that combines them in a joint
frequency–severity framework.

# Results and interpretation

- Summarise the results obtained from both models. 
- Provide a concise interpretation of the empirical findings, 
- focusing on substantive implications

## Summary

Insert table of coefficients for both here

As seen in Table \@ref(tab:coeftableexpendaturetwopart), the results indicate...


```{r coeftableexpendaturetwopart, results='asis', message=FALSE, warning=FALSE, echo=FALSE}
#---------------- Expendature two part table (semi-automated) -----------------#
# qol function for this automated table
format_p_vec <- function(x) {
  x_num <- as.numeric(x)
  out <- rep("", length(x_num))
  non_na <- !is.na(x_num)
  vals <- x_num[non_na]
  out[non_na] <- ifelse(vals < 0.001, "<0.001", sprintf("%.3f", vals))
  out
}

# tidy & join 
pt1 <- broom::tidy(m_part1) %>%
  select(term, estimate, std.error, p.value) %>%
  rename(pt1_est = estimate, pt1_se = std.error, pt1_p = p.value)

pt2 <- broom::tidy(m_part2) %>%
  select(term, estimate, std.error, p.value) %>%
  rename(pt2_est = estimate, pt2_se = std.error, pt2_p = p.value)

raw_data <- full_join(pt1, pt2, by = "term")

#  create groups and clean labels

clean_data <- raw_data %>%
  mutate(
    # define gropu names
    var_group = case_when(
      term == "(Intercept)" ~ "Intercept",
      
      # --- Continuous / Non-Categorical Variables ---
      # grouped together to be sorted, but hide header
      str_starts(term, "age") ~ "Non_Categorical", 
      str_starts(term, "bmi") ~ "Non_Categorical",
      str_starts(term, "income")  ~ "Non_Categorical",
      str_starts(term, "ndvisit") ~ "Non_Categorical",
      
      # --- Categorical Variables (w. explicit reference headers) ---
      str_starts(term, "education") ~ "Education (ref: Less than High School)",
      str_starts(term, "ethnicity") ~ "Ethnicity (ref: White)",
      str_starts(term, "gender")    ~ "Gender (ref: Female)",
      str_starts(term, "general")   ~ "General Health (ref: Excellent)", 
      str_starts(term, "hyperlipidemia") ~ "Hyperlipidemia", 
      str_starts(term, "hypertension")   ~ "Hypertension",
      str_starts(term, "mental")    ~ "Mental Health (ref: Excellent)",
      str_starts(term, "region")    ~ "Region (ref: Northeast)",
      TRUE ~ "Other"
    ),
    
    # define row labs 
    label = case_when(
      term == "(Intercept)" ~ "(Intercept)",
      
      # continuous
      str_starts(term, "age") ~ "age",
      str_starts(term, "bmi") ~ "bmi",
      str_starts(term, "income")  ~ "income",
      str_starts(term, "ndvisit") ~ "ndvisit", 
      
      # categorical
      # striping 'education_cat_detailed' to reduce table width
      str_starts(term, "education_cat_detailed") ~ str_remove(term, "education_cat_detailed"),
      str_starts(term, "ethnicity") ~ str_remove(term, "ethnicity"),
      str_starts(term, "gender")    ~ str_remove(term, "gender"),
      str_starts(term, "general")   ~ str_remove(term, "general"),
      str_starts(term, "hyperlipidemia") ~ str_remove(term, "hyperlipidemia"),
      str_starts(term, "hypertension")   ~ str_remove(term, "hypertension"),
      str_starts(term, "mental")    ~ str_remove(term, "mental"),
      str_starts(term, "region")    ~ str_remove(term, "region"),
      TRUE ~ term
    ),
    
    # custom order for rows
    label_rank = case_when(
      # Education Order
      label == "High School Graduate" ~ 1,
      label == "Some College"         ~ 2,
      label == "College Graduate"     ~ 3,
      label == "Post-Graduate"        ~ 4,
      
      # Health Order
      label == "Poor"  ~ 1,
      label == "Fair"  ~ 2,
      label == "Good"  ~ 3,
      label == "VGood" ~ 4,
      
      TRUE ~ 10 
    )
  ) 

# more group ordering tweaking

all_groups <- unique(clean_data$var_group)

# !note: must match the new text strings exactly for sorting to work
top_groups <- c(
  "Intercept", 
  "Non_Categorical", 
  "General Health (ref: Excellent)", 
  "Mental Health (ref: Excellent)"
)

# combine top groups & add rest (alphabetical)
final_group_order <- c(
  top_groups, 
  sort(setdiff(all_groups, top_groups))
)

sorted_data <- clean_data %>%
  # var_group into a factor with our custom order
  mutate(var_group = factor(var_group, levels = final_group_order)) %>%
  # arrange by group, then by custom rank, then alphabetically
  arrange(var_group, label_rank, label) %>%
  # format
  mutate(
    pt1_est = ifelse(is.na(pt1_est), "", sprintf("%.3f", pt1_est)),
    pt1_se  = ifelse(is.na(pt1_se),  "", sprintf("%.3f", pt1_se)),
    pt2_est = ifelse(is.na(pt2_est), "", sprintf("%.3f", pt2_est)),
    pt2_se  = ifelse(is.na(pt2_se),  "", sprintf("%.3f", pt2_se)),
    pt1_p   = format_p_vec(pt1_p),
    pt2_p   = format_p_vec(pt2_p)
  )

# Kable 

group_indices <- sorted_data %>%
  mutate(row = row_number()) %>%
  group_by(var_group) %>%
  summarise(start = min(row), end = max(row)) %>%
  arrange(start) 

kable_data <- sorted_data %>% 
  select(label, pt1_est, pt1_se, pt1_p, pt2_est, pt2_se, pt2_p)

colnames(kable_data) <- c(
  "Term",
  "Estimate", "Std. Error", "p-value",
  "Estimate ", "Std. Error ", "p-value "
)

header_labels <- c(
  " " = 1, 
  "Pt 1: Prob. of any expense" = 3, 
  "Pt 2: Expenditure amount"   = 3
)

beta_symbol <- ifelse(knitr::is_latex_output(), "$\\hat{\\beta}$", "&beta;")

caption_text <- paste0(
  "Estimated coefficients (", beta_symbol, "), standard errors, and p-values. ",
  "A Probit and Gamma GLM (log link) were used in combination to model the probability of incurring any expense (Pt 1) and the expenditure among health care users (Pt 2). ",
  "Variable selection was performed independently for Pt 1 and Pt 2."
)

if (knitr::is_latex_output()) {
  cat("\\clearpage\n")
  cat("\\begingroup\n")
  cat("\\renewcommand{\\arraystretch}{1.2}\n")
}

coeffs_expend <- knitr::kable(
  kable_data,
  format   = ifelse(knitr::is_latex_output(), "latex", "html"),
  booktabs = knitr::is_latex_output(),
  caption  = caption_text,
  align    = c("l", rep("r", ncol(kable_data) - 1)),
  linesep  = "", 
  escape   = !knitr::is_latex_output(),
  table.placement = "H"
) %>%
  add_header_above(header_labels, escape = !knitr::is_latex_output()) %>%
  kable_styling(full_width = FALSE, position = "center")

for(i in 1:nrow(group_indices)) {
  group_name <- as.character(group_indices$var_group[i])
  if(group_name %in% c("Intercept", "Non_Categorical")) next 
  coeffs_expend <- coeffs_expend %>%
    pack_rows(group_name, group_indices$start[i], group_indices$end[i])
}

coeffs_expend

if (knitr::is_latex_output()) {
  cat("\\endgroup\n")
  cat("\\clearpage\n")
}
```


## Expenditure


```{r expenditure-predictions, echo=FALSE}
#### Predictions
####
####
# --- Step 1: Predict Probability (Part 1) ---
# type = "response" gives us a probability between 0 and 1
prob_any_expense <- predict(m_part1, newdata = meps_clean, type = "response")

# --- Step 2: Predict Conditional Cost (Part 2) ---
# We predict cost for EVERYONE as if they did go to the doctor.
# type = "response" automatically undoes the Log-link (returns dollars, not log-dollars)
cond_cost <- predict(m_part2, newdata = meps_clean, type = "response")

# --- Step 3: Combine them ---
# Expected Expenditure = Probability * Cost
expected_expenditure <- prob_any_expense * cond_cost

# --- Step 4: Add to your dataframe to compare ---
meps_results <- meps_clean %>%
  mutate(
    pred_prob = prob_any_expense,
    pred_cost_if_seen = cond_cost,
    final_prediction = expected_expenditure,
    actual_cost = dvexpend
  )

# Inspect the first few rows
head(
  meps_results %>% select(has_expense, actual_cost, pred_prob, final_prediction)
)

# evaluation of predictions
meps_results$error <- meps_results$actual_cost - meps_results$final_prediction

# 1. RMSE (Root Mean Squared Error)
rmse <- sqrt(mean(meps_results$error^2))

# 2. MAE (Mean Absolute Error)
mae <- mean(abs(meps_results$error))

# Print results
paste0("RMSE: $", round(rmse, 2))
paste0("MAE:  $", round(mae, 2))

ggplot(meps_results, aes(x = final_prediction, y = actual_cost)) +
  geom_point(alpha = 0.3, color = "blue") + # Semi-transparent dots
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + # Ideally where points should fall
  labs(
    title = "Actual vs. Predicted Expenditure",
    x = "Predicted Cost ($)",
    y = "Actual Cost ($)"
  ) +
  theme_minimal()
# +
#   coord_cartesian(xlim = c(0, 10000), ylim = c(0, 10000))

total_actual <- sum(meps_results$actual_cost)
total_pred <- sum(meps_results$final_prediction)
bias_percent <- ((total_pred - total_actual) / total_actual) * 100

paste0("Total Actual Cost:    $", format(round(total_actual), big.mark = ","))
paste0("Total Predicted Cost: $", format(round(total_pred), big.mark = ","))
paste0("Bias: ", round(bias_percent, 2), "%")

# Calculate the errors (Residuals)
residuals <- meps_results$actual_cost - meps_results$final_prediction

# 1. The IQR (Range of the middle 50%)
iqr_metric <- IQR(residuals)

# 2. Specific Percentiles (e.g., The range for 90% of patients)
quantiles <- quantile(
  residuals,
  probs = c(0.01, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 0.99)
)

print(paste0("IQR of Error: $", round(iqr_metric, 2)))
print(quantiles)

# boxplot
meps_results <- meps_results %>%
  mutate(
    cost_group = cut(
      actual_cost,
      breaks = c(-Inf, 0, 100, 1000, 5000, 10000, 20000, 40000, Inf),
      labels = c(
        "Zero",
        "$1-100",
        "$100-1k",
        "$1k-5k",
        "$5k+",
        "$10k+",
        "$20k+",
        "$40k+"
      )
    )
  )

# Plot the errors for each group
ggplot(meps_results, aes(x = cost_group, y = error)) +
  geom_boxplot(fill = "lightblue", alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Distribution of Prediction Errors by Cost Category",
    subtitle = "Box shows the IQR (Middle 50% of errors)",
    x = "Actual Expenditure Category",
    y = "Prediction Error ($)"
  ) +
  theme_minimal()
# +
# coord_cartesian(ylim = c(-2000, 2000)) # Zoom in to ignore extreme outliers

```


## Utilisation

## Summary 

## Interpretation

# Discussion 

## Analysis & evaluation

Limitations, analysis and evaluations 

Mention connections between the two here - it's heavily embedded

## Expansion 

# Drafting

The probit models the probability of having any expense, 

\begin{equation}
  (\#eq:probit)
  \mathbb{P}[Y>0]
\end{equation}


The Gamma GLM (with a log-link) models the conditional mean of the expense, 
given that it is positive, 
\begin{equation}
  \mathbb{E}[Y \mid Y>0]
\end{equation}

The 

Therefore expected cost for a given individual is, under this model, given by
\begin{equation}
  \mathbb{E}(Y_i \mid \mathbf{x}_i)
  = \mathbb{P}(Y_i > 0 \mid \mathbf{x}_i)\,
    \mathbb{E}(Y_i \mid Y_i > 0, \mathbf{x}_i).
\end{equation}

\@ref(eq:probit)

```{=latex}
\newpage
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}
```

# (APPENDIX) Appendices {-} 

<!-- Placeholder code from CS1 project -->
<!-- # Bonus exploration: R Shiny dashboard {#apd-dashboard} -->

<!-- Because there is no corner of our souls we would not turn over for extra credit, -->
<!-- we conducted an additional exploration of an area for future study identified in -->
<!-- the conclusion.  Rather than re-running the analyses for multiple indices, we -->
<!-- developed a dashboard that allows the same workflow to be applied quickly to -->
<!-- other stock market indices. -->

<!-- As R Markdown was used to write up this project, one natural approach would have -->
<!-- been simply to update the ticker symbol and render a new R Markdown document for -->
<!-- each index. While this would work in principle, it is not fully satisfactory: -->
<!-- the document takes time to render (particularly with 50,000-repetition bootstrap -->
<!-- simulations) and each iteration would require manual narrative adjustments. To -->
<!-- address this, we developed an interactive R Shiny dashboard, hosted on -->
<!-- https://www.shinyapps.io/, which provides a dynamic interface for rapid -->
<!-- comparative analysis. The dashboard is available via: -->

<!-- - https://3enji.shinyapps.io/SMM047-202526-Group07-Dashboard/ -->

<!-- Figure \@ref(fig:appendixdashboard1) shows the dashboard start page. Clicking -->
<!-- Run Analysis loads information for the entered Yahoo Finance ticker (with the -->
<!-- default set to `^IXIC`). -->

<!-- Figure \@ref(fig:appendixdashboard2) presents an annotated view of the user -->
<!-- interface.  The dashboard includes (1) a Ticker Selection field that accepts -->
<!-- standard Yahoo Finance symbols; (2–4) Data Cleaning controls, where users can -->
<!-- define the study range and mask a specific outlier period (2020-02-24 to 2020-03-23 in the case of our report); (5) Bootstrap Settings, -->
<!-- allowing users to choose the number of repetitions (to balance accuracy and  -->
<!-- computation time); and (6) Analysis Tabs, which display the cleaned data, -->
<!-- normality tests, and independence diagnostics. -->

<!-- Future development may include support for multiple exclusion windows and -->
<!-- optimisation of the bootstrapping algorithm, which is currently -->
<!-- resource-intensive for large numbers of repetitions. While the present -->
<!-- implementation mirrors the project’s core analysis, there is also scope to -->
<!-- extend this to incorporate additional methods, such as assessing normality using -->
<!-- bootstrapped kurtosis estimates and adding functionality to vary the m value in -->
<!-- the m-out-of-n bootstrap (currently hard coded as m=13). In addition to the -->
<!-- adjusted closing price used in this project, future enhancements could also -->
<!-- allow users to select alternative price series (for example, open, close, or -->
<!-- intraday mid prices) and different aggregation frequencies (for example, daily -->
<!-- or weekly) within the app. -->


```{=latex}
\clearpage
\newpage
```

# Reproducibility, accessibility & declarations (Gen-AI & word count)
<!-- BE note: taken out as we are submitting R file separately -->
<!-- Use: `knitr::purl("cs1-group07.rmd", documentation = 0)` to generate r file -->
<!-- Use: `tools::showNonASCIIfile("cs1-group07.rmd")` to debug -->
<!-- ## R Code Documentation -->

## Reproducibility & accessibility 

An accessible HTML version of this report is available via a public GitHub page:

- !TBI

This report was created in R Markdown. The source code is open source and is
available via:

- !TBI

Changes made to this document were tracked using Git and are also available via
the same repository
(!TBI). 

A bonus interactive dashboard is available via:

- TBI

The source code for this dashboard is available via the project repository: 

- TBI

# R Code

```{=latex}
\small
```
<!-- # ```{r r-code-appendix, ref.label = grep("^setup-", knitr::all_labels(), value = TRUE, invert = TRUE), echo = TRUE, eval = FALSE} -->
<!-- # ``` -->
```{=latex}
\normalsize
```

---
